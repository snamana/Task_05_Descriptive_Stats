# Research Task 5 Summary: Descriptive Statistics & LLMs

## Overview

In this research task, I used statistics from the SU Men’s Basketball 2024–25 season and tested how accurately a large language model (ChatGPT) could interpret, reason through, and answer natural language questions about that dataset.

## Key Objectives

- Ask both basic and analytical questions to an LLM
- Engineer prompts to improve answer quality
- Validate correctness against real data
- Reflect on success/failure of LLM responses

---

## Observations

### What Worked Well

- ChatGPT answered simple factual questions (games played, top scorer) correctly
- When structured input (like tables or metrics) was provided, it could explain reasoning well
- With iterative prompting, it gave thoughtful advice on strategy (offense vs. defense)
- Could simulate coach-style reasoning when given context

### What Didn’t Work

- The LLM struggled when questions were vague (e.g., “Who was most improved?”)
- It made assumptions or gave generic answers without proper context
- Needed guidance for anything that required comparing or calculating

---

## Lessons Learned

- Prompt design is critical — giving the LLM numbers or structure transforms performance
- Even strong LLMs won’t "reason" on stats unless we guide them step-by-step
- LLMs are best when used as *reasoning tools*, not number crunchers

---

## Final Thoughts

This task was more than testing LLMs — it was about learning how to collaborate with one. The model can be an insightful assistant when paired with human validation and strong input structuring.

